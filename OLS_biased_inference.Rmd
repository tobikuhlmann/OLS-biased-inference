---
title: "Stat625 Project Fall 2018:  \n What happens to OLS if core assumptions are not met?"
author: "Tobias Kuhlmann and Yuting Xu"
date: "29/10/2018"
output: 
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Robust Estimation and Prediction: What happens to OLS if core assumptions are not met?

## Theoretical Study

##### Heteroscedasticity (Steady variance over time)

- Summary
 - Still unbiased coefficient estimation, but inefficient (Efficiency in this context describes how fast the estimated coefficient converges to the real data generating coefficient), no minimum variance estimator
 - Biased standard error estimation

- unequal error variances, the ordinary least squares estimators of the regression coefficients are still unbiased and consistent, but they are no longer minimum variance estimators. Also, (12{b} is no longer given by a 2(X'X)-I. Need a full covariance matrix.
- The Assumption of Homoscedasticity (OLS Assumption 5) – If errors are heteroscedastic (i.e. OLS assumption is violated), then it will be difficult to trust the standard errors of the OLS estimates. Hence, the confidence intervals will be either too narrow or too wide. Also, violation of this assumption has a tendency to give too much weight on some portion (subsection) of the data. Hence, it is important to fix this if error variances are not constant. You can easily check if error variances are constant or not. Examine the plot of residuals predicted values or residuals vs. time (for time series models). Typically, if the data set is large, then errors are more or less homoscedastic. If your data set is small, check for this assumption.

- If the error term is not homoscedastic (we use the residuals as a proxy for the unobservable error term), the OLS estimator is still consistent and unbiased but is no longer the most efficient in the class of linear estimators. It is the GLS estimator now that enjoys this property.

- Although the OLS coefficient estimations remain unbiased when applied to heteroscedastic data, they become inefficient. Efficiency in this context describes how fast the estimated coefficient converges to the real data generating coefficient. The coefficient estimates can be inefficiently high (low), if highly volatile observations in the data tend to be positive (negative)

- Homoscedasticity (Constant Variance in the error term): The violation of homoscedasticity (called as heteroscedasticity) causes the regression coefficient produced by OLS will be less reliable because the data point across each independent variable will not influence equally. Heteroscedasticity also makes difficult to forecast true standard error that makes the confidence interval of the regression coefficient will be large.

- False positive significance: While heteroscedasticity does not cause bias in the coefficient estimates, it does make them less precise. Lower precision increases the likelihood that the coefficient estimates are further from the correct population value. Heteroscedasticity tends to produce p-values that are smaller than they should be. This effect occurs because heteroscedasticity increases the variance of the coefficient estimates but the OLS procedure does not detect this increase. Consequently, OLS calculates the t-values and F-values using an underestimated amount of variance. This problem can lead you to conclude that a model term is statistically significant when it is actually not significant.

- Goes into signal noise theory: Observations with large variances have a high impact on coefficients, but are often much noise, less signal.


##### Autocorrelation in error terms

- Summary
 - Still unbiased coefficient estimation, but not the most efficient anymore (GLS)
 - Biased standard error estimation
 - MSE will underestimate true error variance

- While it does not bias the OLS coefficient estimates, the standard errors tend to be underestimated (and the t-scores overestimated) when the autocorrelations of the errors at low lags are positive.
- If you do not correct for autocorrelation, then OLS estimates won’t be BLUE, and they won’t be reliable enough.


##### Solution 
- Important to know that forecasting still possible, but inference from standard error are biased
- Newey–West standard errors account for autocorrelation and heteroscedasticity
- GLS 
- AR/MA/ARMA Modeling for time series
- Lags, First Differences, Logarithms, and Growth Rates


Mathematical notation: Chapter 12, page 486, use that kind of notations


References
-------------
- https://www.albert.io/blog/key-assumptions-of-ols-econometrics-review/
- https://stats.stackexchange.com/questions/188664/how-incorrect-is-a-regression-model-when-assumptions-are-not-met
- https://www.quora.com/What-are-the-consequences-of-violating-linear-regression-assumptions
- https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2561112
- https://en.wikipedia.org/wiki/Autocorrelation#Regression_analysis
- https://en.wikipedia.org/wiki/Heteroscedasticity
- http://statisticsbyjim.com/regression/heteroscedasticity-regression/
- https://www.jstor.org/stable/1913610?seq=1#metadata_info_tab_contents
- Applied linear statistical models (5th edition, Kutner, Nachtsheim, Neter, Li), Chapter 11 and 12
Robust standard errors:
- https://cran.r-project.org/web/packages/sandwich/sandwich.pdf
- https://www.rdocumentation.org/packages/sandwich/versions/2.5-0/topics/NeweyWest


## Approach
Simulate autocorrelated and heteroscedastic data and fit OLS models to it. Compare OLS inference and inference from OLS with corrected standard errors and advanced models like ARMA / ARIMA.

Also, we can directly simulate ARMA Processes and show the differences between them and OLS estimates on them.

For Corrected standard errors (Newey West procedure) in R : 
- https://cran.r-project.org/web/packages/sandwich/sandwich.pdf
- https://www.rdocumentation.org/packages/sandwich/versions/2.5-0/topics/NeweyWest

##### Init
```{r 0}
library("sandwich")
library("lmtest")
library("investr")
library("tidyr")
library("dotwhisker")
library("broom")
library("dplyr")

set.seed(1)
```

##### Function 
```{r 0.1}
# inspired by https://stats.stackexchange.com/questions/231632/how-to-plot-simultaneous-and-pointwise-confidence-bands-for-linear-regression-wi
simultaneous_CBs <- function(linear_model, newdata, level = 0.95, weights=NULL){
    # Working-Hotelling 1 – α confidence bands for the model linear_model
    # at points newdata with α = 1 - level

    # estimate of residual standard error
    lm_summary <- summary(linear_model)
    # degrees of freedom 
    p <- lm_summary$df[1]
    # residual degrees of freedom
    nmp <-lm_summary$df[2]
    # F-distribution
    Fvalue <- qf(level,p,nmp)
    # multiplier
    W <- sqrt(p*Fvalue)
    # confidence intervals for the mean response at the new points
    CI <- predict(linear_model, newdata, se.fit = TRUE, interval = "confidence", level = level, weights=weights)
    # mean value at new points
    Y_h <- CI$fit[,1]
    # Working-Hotelling 1 – α confidence bands
    LB <- Y_h - W*CI$se.fit
    UB <- Y_h + W*CI$se.fit
    sim_CB <- data.frame(LowerBound = LB, Mean = Y_h, UpperBound = UB)
}
```

## Calculation of NeweyWest corrected standard errors for mean responses

- NeweWest gives us variance covariance matrix of coefficients
  - Newey-West multiplies the covariance term of lag j (e.g. εt εt-j ) by the weight
[1-j/(M+1)], where M is the specified maximum lag

- How to get corrected mean response standard errors (for confidence intervals / bands) from NeweyWest covariance matrix
$$\hat{Y_n} = b_0+b_1*X$$
$$se(\hat{Y_n}) = se(b_0+b_1*X)$$
$$Var(b_0+b_1*X) = Var(b_0) + X^2 * Var(b_1) + 2XCov(b_0, b_1)$$

```{r 0.2}
simultaneous_adjusted_CBs <- function(linear_model, newdata, level = 0.95, weights=NULL){
    # Working-Hotelling 1 – α NeweyWest adjusted confidence bands for the model linear_model
    # at points newdata with α = 1 - level
    
    # estimate of residual standard error
    lm_summary <- summary(linear_model)
    # degrees of freedom 
    p <- lm_summary$df[1]
    # residual degrees of freedom
    nmp <-lm_summary$df[2]
    # F-distribution
    Fvalue <- qf(level,p,nmp)
    # multiplier
    W <- sqrt(p*Fvalue)
    # Newey & West (1994) corrected covariance matrix
    newey_west_vcov <- NeweyWest(linear_model)
    se_y_squared <- newey_west_vcov[1,1] + newey_west_vcov[2,2] * newdata$x^2 + 2 * newdata$x * newey_west_vcov[2,1]
    # confidence intervals for the mean response at the new points
    CI <- predict(linear_model, newdata, se.fit = TRUE, interval = "confidence", level = level, weights=weights)
    # mean value at new points
    Y_h <- CI$fit[,1]
    # Working-Hotelling 1 – α confidence bands
    LB <- Y_h - W*sqrt(se_y_squared)
    UB <- Y_h + W*sqrt(se_y_squared)
    sim_CB <- data.frame(LowerBound = LB, Mean = Y_h, UpperBound = UB)
}
```

## Heteroscedasticity
Nonconstant variance over time

```{r 1}
# simulate data with increasing variance
x <- runif(100, 0, 10)
y <- rnorm(100, 0, x)

# weights vector for weighted least squares
weights = 1/x

# fit linear model
data.lm = lm(y ~ x, data=data.frame(cbind(y,x)))
data.wlm = lm(y ~ x, data=data.frame(cbind(y,x)), weights=weights)

summary(data.lm)
summary(data.wlm)

# Unorrected standard errors
# -------------------------
vcov(data.lm)

# Corrected standard errors
# -------------------------
# Newey & West (1994)
NeweyWest(data.lm)

# Inference: Uncorrected
# -------------------------
# coefficients
coefci(data.lm, level=0.95)

# regression band ols
CB = simultaneous_CBs(data.lm, data.frame(x=sort(x)), level = 0.95)


# Inference: Corrected
# -------------------------
# coefficients
coefci(data.lm, level=0.95, vcov = NeweyWest)

# confidence bands weighted regression wls
CB_weighted = simultaneous_CBs(data.wlm, data.frame(x=sort(x)), level = 0.95)

# regression band
CB_NeweyWest = simultaneous_adjusted_CBs(data.lm, data.frame(x=sort(x)), level = 0.95)


# Plot data
# -------------------------
# plot regression confidence bands
plot <- plot(x, y)
abline(data.lm) 
lines(x=sort(x), y=CB$LowerBound, col="red") 
lines(x=sort(x), y=CB$UpperBound, col="red")
#lines(x=sort(x), y=CB_weighted$LowerBound, col='blue') 
#lines(x=sort(x), y=CB_weighted$UpperBound, col='blue')
lines(x=sort(x), y=CB_NeweyWest$LowerBound, col='green') 
lines(x=sort(x), y=CB_NeweyWest$UpperBound, col='green')
legend("topleft",legend=c("OLS", "OLS Newey West"),col = c("red", "green"),lty = 2)

# check with investr build in working hotelling function
#plotFit(data.lm, interval = 'confidence', adjust = 'Scheffe', main = 'Working-Hotelling DelTime ~ Distance')
# check succesfull
#lines(x=sort(x), y=CB_weighted$LowerBound, col='blue') 
#lines(x=sort(x), y=CB_weighted$UpperBound, col='blue')
#lines(x=sort(x), y=CB_NeweyWest$LowerBound, col='green') 
#lines(x=sort(x), y=CB_NeweyWest$UpperBound, col='green')
#legend("topleft",legend=c("OLS","WLS", "OLS Newey West"),col = c("black","blue", "green"),lty = 2)


# Advanced models
# -------------------------
```

```{r coefciplot1}
# regression compatible with tidy
m1_df <- tidy(data.lm) %>% filter(term != "(Intercept)") %>% mutate(model = "Standard")
m2_df <- tidy(data.lm) %>% filter(term != "(Intercept)") %>% mutate(model = "NeweyWest")  %>% mutate(std.error = sqrt(NeweyWest(data.lm)[2,2]))
m2_df
# change standard errors to newey west

two_models <- rbind(m1_df, m2_df)

dwplot(two_models) + 
        theme_bw() + xlab("Value") + ylab("") +
        ggtitle("Slope Confidence Intervals")
```



## Autocorrelation
```{r 2}
# Autocorrelation
x <- rnorm(500,1,1)
# Simulate ARIMA(1,0,0): (p, d, q) are the AR order, the degree of differencing, and the MA order
eps <- arima.sim(list(order = c(1,0,0), ar = 0.7), n = 500, sd=1)
# plot(eps)
b0 <- 1
b1 <- 1
y = b0 + b1*x + eps

# analyze autocorrelation
#acf(y, type=c("correlation"), plot=FALSE, demean=FALSE)
acf(y, type=c("correlation"), plot=TRUE, demean=FALSE)

# fit linear model
data.lm = lm(y ~ x, data=data.frame(cbind(y,x)))

# Unorrected standard errors
# -------------------------
vcov(data.lm)

# Corrected standard errors
# -------------------------
# Newey & West (1994)
NeweyWest(data.lm)

# Inference: Uncorrected
# -------------------------
# coefficients
coefci(data.lm, level=0.95)

# regression band
CB = simultaneous_CBs(data.lm, data.frame(x=sort(x)), level = 0.95)

# Inference: Corrected
# -------------------------
# coefficients
coefci(data.lm, level=0.95, vcov = NeweyWest)


# regression band
CB_NeweyWest = simultaneous_adjusted_CBs(data.lm, data.frame(x=sort(x)), level = 0.95)
 

# plot data
plot(x, y, ylim = c(min(CB_NeweyWest$LowerBound), max(CB_NeweyWest$UpperBound)))
# plot confidence bands
abline(data.lm) 
lines(x=sort(x), y=CB$LowerBound, col='red') 
lines(x=sort(x), y=CB$UpperBound, col='red')
lines(x=sort(x), y=CB_NeweyWest$LowerBound, col='green') 
lines(x=sort(x), y=CB_NeweyWest$UpperBound, col='green')
legend("topleft",legend=c("OLS", "OLS Newey West"),col = c("red", "green"),lty = 2)

# Advanced models


```
```{r coefciplot2}
# regression compatible with tidy
m1_df <- tidy(data.lm) %>% filter(term != "(Intercept)") %>% mutate(model = "Standard")
m2_df <- tidy(data.lm) %>% filter(term != "(Intercept)") %>% mutate(model = "NeweyWest")  %>% mutate(std.error = sqrt(NeweyWest(data.lm)[2,2]))
m2_df
# change standard errors to newey west

two_models <- rbind(m1_df, m2_df)
two_models
dwplot(two_models) + 
        theme_bw() + xlab("Value") + ylab("") +
        ggtitle("Slope Confidence Intervals")
```



## Autocorrelation and Heteroscedasticity
```{r 3}
# simulate Brownian motion (stock price process)
x <- sort(runif(100, 0, 1))
y <- diffinv(rnorm(99))

# analyze autocorrelation
acf(y, type=c("correlation"), plot=FALSE, demean=FALSE)
acf(y, type=c("correlation"), plot=TRUE, demean=FALSE)

# fit linear model
data.lm = lm(y ~ x, data=data.frame(cbind(y,x)))

# Unorrected standard errors
# -------------------------
vcov(data.lm)

# Corrected standard errors
# -------------------------
# Newey & West (1994)
NeweyWest(data.lm)

# Inference: Uncorrected
# -------------------------
# coefficients
coefci(data.lm, level=0.95)

# regression band
CB = simultaneous_CBs(data.lm, data.frame(x=sort(x)), level = 0.95)

# Inference: Corrected
# -------------------------
# coefficients
coefci(data.lm, level=0.95, vcov = NeweyWest)

# regression band
CB_NeweyWest = simultaneous_adjusted_CBs(data.lm, data.frame(x=sort(x)), level = 0.95)
 

# plot data
plot(x, y, ylim = c(min(CB_NeweyWest$LowerBound), max(CB_NeweyWest$UpperBound)))
# plot confidence bands
abline(data.lm) 
lines(x=sort(x), y=CB$LowerBound, col='red') 
lines(x=sort(x), y=CB$UpperBound, col='red')
lines(x=sort(x), y=CB_NeweyWest$LowerBound, col='green') 
lines(x=sort(x), y=CB_NeweyWest$UpperBound, col='green')
legend("topleft",legend=c("OLS", "OLS Newey West"),col = c("red", "green"),lty = 2)



# Advanced models



```
```{r coefciplot3}
# regression compatible with tidy
m1_df <- tidy(data.lm) %>% filter(term != "(Intercept)") %>% mutate(model = "Standard")
m2_df <- tidy(data.lm) %>% filter(term != "(Intercept)") %>% mutate(model = "NeweyWest")  %>% mutate(std.error = sqrt(NeweyWest(data.lm)[2,2]))
m2_df
# change standard errors to newey west

two_models <- rbind(m1_df, m2_df)

dwplot(two_models) + 
        theme_bw() + xlab("Value") + ylab("") +
        ggtitle("Slope Confidence Intervals")
```

## Autocorrelation and Heteroscedasticity

```{r 4}
# AR(1) model with heteroscedasticity and autocorrelation
x <- rnorm(500,1,1)
b0 <- 1
b1 <- 2
h <- function(x) return(1+0.9*x) # add heteroscedasticity
eps = rnorm(500,0,h(x))
y = b0 + b1*x + eps

# weights vector for weighted least squares
weights = 1/h(x)

# analyze autocorrelation
# acf(y, type=c("correlation"), plot=TRUE, demean=FALSE)


# fit linear model
data.lm = lm(y ~ x, data=data.frame(cbind(y,x)))
data.wlm = lm(y ~ x, data=data.frame(cbind(y,x)), weights=weights)

summary(data.lm)

# Unorrected standard errors
# -------------------------
vcov(data.lm)

# Corrected standard errors
# -------------------------
# Newey & West (1994)
NeweyWest(data.lm)

# Inference: Uncorrected
# -------------------------
# coefficients
coefci(data.lm, level=0.95)

# regression band
CB = simultaneous_CBs(data.lm, data.frame(x=sort(x)), level = 0.95)

# Inference: Corrected
# -------------------------
# coefficients
coefci(data.lm, level=0.95, vcov = NeweyWest)

# WLS confidence bands with standard error from data generating process
CB_weighted = simultaneous_CBs(data.wlm, data.frame(x=sort(x)), level = 0.95, weights=weights)

# Newey West regression band
CB_NeweyWest = simultaneous_adjusted_CBs(data.lm, data.frame(x=sort(x)), level = 0.95)


# plot data
# -------------------------
plot(x, y)
# plot confidence bands
abline(data.lm) 
lines(x=sort(x), y=CB$LowerBound, col="red") 
lines(x=sort(x), y=CB$UpperBound, col="red")
#lines(x=sort(x), y=CB_weighted$LowerBound, col='blue') 
#lines(x=sort(x), y=CB_weighted$UpperBound, col='blue')
lines(x=sort(x), y=CB_NeweyWest$LowerBound, col='green') 
lines(x=sort(x), y=CB_NeweyWest$UpperBound, col='green')
legend("topleft",legend=c("OLS", "OLS Newey West"),col = c("red", "green"),lty = 2)




# Advanced models


```

```{r coefciplot4}
# regression compatible with tidy
m1_df <- tidy(data.lm) %>% filter(term != "(Intercept)") %>% mutate(model = "Standard")
m2_df <- tidy(data.lm) %>% filter(term != "(Intercept)") %>% mutate(model = "NeweyWest")  %>% mutate(std.error = sqrt(NeweyWest(data.lm)[2,2]))
m2_df
# change standard errors to newey west

two_models <- rbind(m1_df, m2_df)

dwplot(two_models) + 
        theme_bw() + xlab("Value") + ylab("") +
        ggtitle("Slope Confidence Intervals")
```

## Smiulation Study Results

- OLS standard erros are wrong in the presence of Heteroscedasticity and Autocorrelation in the data
  - Autocorrelation: Standard OLS erros are to small, Corrected Newey West standard errors are larger, dependend on severity of autocorrelation
  - Heteroscedasticity: Standard OLS erros are to small, Corrected Newey West standard errors are larger, dependend on current variance

## Implications
- Inference from both coefficients and confidence / prediction intervals can get wrong
- Hypothesis test results will have false positivity or false negativity
- Be careful if the data does not fulfil OLS BLUE assumptions. Always use corrected standard errors or more suitable time series models in the presence of autocorrelation or heteroscedasticity!


